{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":189983,"sourceType":"datasetVersion","datasetId":81794}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import save_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:49:02.778834Z","iopub.execute_input":"2024-07-25T12:49:02.779523Z","iopub.status.idle":"2024-07-25T12:49:19.008112Z","shell.execute_reply.started":"2024-07-25T12:49:02.779431Z","shell.execute_reply":"2024-07-25T12:49:19.00701Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_:dict={\"cardboard\":0,\"glass\":2,\"metal\":3,\"paper\":1,\"plastic\":5,\"trash\":4}\nSIZE:tuple=(224,224) # MobileNetV2 typically expects images of size 224x224 pixels","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:49:19.010469Z","iopub.execute_input":"2024-07-25T12:49:19.011154Z","iopub.status.idle":"2024-07-25T12:49:19.017108Z","shell.execute_reply.started":"2024-07-25T12:49:19.011119Z","shell.execute_reply":"2024-07-25T12:49:19.015917Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(path: str,size:tuple=(150,150)) -> tuple:\n    images = []\n    labels = []\n    class_names = []\n    \n    for class_name in os.listdir(path):\n        class_path = os.path.join(path, class_name)\n        if os.path.isdir(class_path):\n            for img_name in tqdm(os.listdir(class_path), desc=f\"Loading {class_name}\"):\n                img_path = os.path.join(class_path, img_name)\n                img = cv2.imread(img_path)\n                if img is not None:\n                    img = cv2.resize(img, size)  # Resize images to 150x150\n                    images.append(img)\n                    labels.append(labels_[class_name])\n    \n    images = np.array(images)\n    labels = np.array(labels)\n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:49:19.018882Z","iopub.execute_input":"2024-07-25T12:49:19.019211Z","iopub.status.idle":"2024-07-25T12:49:19.031916Z","shell.execute_reply.started":"2024-07-25T12:49:19.019183Z","shell.execute_reply":"2024-07-25T12:49:19.03064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## load data","metadata":{}},{"cell_type":"code","source":"imgs_data,labels_data = load_data(\"/kaggle/input/garbage-classification/Garbage classification/Garbage classification\",SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:49:19.033436Z","iopub.execute_input":"2024-07-25T12:49:19.033946Z","iopub.status.idle":"2024-07-25T12:49:42.011506Z","shell.execute_reply.started":"2024-07-25T12:49:19.033906Z","shell.execute_reply":"2024-07-25T12:49:42.010456Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply augmentation to the dataset","metadata":{}},{"cell_type":"code","source":"# Define the image data generator with augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=20,         # Randomly rotate images in the range 0-20 degrees\n    width_shift_range=0.2,     # Randomly shift images horizontally by 20% of the width\n    height_shift_range=0.2,    # Randomly shift images vertically by 20% of the height\n    shear_range=0.2,           # Randomly shear images\n    zoom_range=0.2,            # Randomly zoom into images\n    horizontal_flip=True,      # Randomly flip images horizontally\n    fill_mode='nearest'        # Fill in newly created pixels after rotation or shift\n)\n\n# Augment the dataset\naugmented_images = []\naugmented_labels = []\n\naugmentation_count = int(len(imgs_data) * (1/3))\n\nfor img, label in zip(imgs_data, labels_data):\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    i = 0\n    for batch in datagen.flow(img, batch_size=1):\n        augmented_images.append(batch[0].astype('uint8'))\n        augmented_labels.append(label)\n        i += 1\n        if i >= (augmentation_count / len(imgs_data)):\n            break\n\n# Convert lists to numpy arrays\naugmented_images = np.array(augmented_images)\naugmented_labels = np.array(augmented_labels)\n\n# Combine the original and augmented data\nfinal_imgs_data = np.concatenate((imgs_data, augmented_images), axis=0)\nfinal_labels_data = np.concatenate((labels_data, augmented_labels), axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:49:42.014255Z","iopub.execute_input":"2024-07-25T12:49:42.014645Z","iopub.status.idle":"2024-07-25T12:50:12.383422Z","shell.execute_reply.started":"2024-07-25T12:49:42.014613Z","shell.execute_reply":"2024-07-25T12:50:12.382098Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Size before augmentation : \",imgs_data.shape[0])\nprint(\"Size After augmentation : \",final_imgs_data.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:12.385054Z","iopub.execute_input":"2024-07-25T12:50:12.385491Z","iopub.status.idle":"2024-07-25T12:50:12.392123Z","shell.execute_reply.started":"2024-07-25T12:50:12.385455Z","shell.execute_reply":"2024-07-25T12:50:12.39064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Display images","metadata":{}},{"cell_type":"code","source":"def show_random_images(images: np.ndarray, labels: np.ndarray, num_samples: int, mode: str = 'BGR2RGB'):\n    if num_samples > len(images):\n        raise ValueError(\"Number of samples requested exceeds available samples.\")\n    \n    # Randomly select indices\n    random_indices = random.sample(range(len(images)), num_samples)\n    \n    class_names={j:i for i,j in labels_.items()}\n    \n    selected_images = images[random_indices]\n    selected_labels = [class_names[labels[index]].capitalize() for index in random_indices]\n\n    # Define the number of rows and columns for displaying images\n    num_cols = 3\n    num_rows = (num_samples + num_cols - 1) // num_cols\n\n    plt.figure(figsize=(15, 5 * num_rows))\n\n    for i, (img, label) in enumerate(zip(selected_images, selected_labels)):\n        plt.subplot(num_rows, num_cols, i + 1)\n        if isinstance(mode,str) and not mode is None:\n            mode=mode.upper()\n        if mode == 'GRAY':\n            img_display = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            plt.imshow(img_display, cmap='gray')\n        elif mode == 'BGR2RGB':\n            img_display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            plt.imshow(img_display)\n        elif mode == 'RGB':\n            plt.imshow(img)\n        else:\n            raise ValueError(f\"Unsupported mode: {mode}\")\n        plt.title(label)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:12.394119Z","iopub.execute_input":"2024-07-25T12:50:12.394575Z","iopub.status.idle":"2024-07-25T12:50:12.412625Z","shell.execute_reply.started":"2024-07-25T12:50:12.394536Z","shell.execute_reply":"2024-07-25T12:50:12.411186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_image(images: np.ndarray, labels: np.ndarray, index: int, mode: str = 'RGB'):\n    if index >= len(images):\n        raise ValueError(\"Index is out of range for images array.\")\n    \n    # Mapping of class indices to class names (assuming labels are numeric)\n    class_names={j:i for i,j in labels_.items()}  # Replace with your actual class names or logic\n    \n    img = images[index]\n    label = class_names[labels[index]]\n\n    plt.figure(figsize=(5, 5))\n    \n    if isinstance(mode, str) and mode.upper() == 'GRAY':\n        img_display = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        plt.imshow(img_display, cmap='gray')\n    elif isinstance(mode, str) and mode.upper() == 'BGR2RGB':\n        img_display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.imshow(img_display)\n    elif isinstance(mode, str) and mode.upper() == 'RGB':\n        plt.imshow(img)\n    else:\n        raise ValueError(f\"Unsupported mode: {mode}\")\n    \n    plt.title(label.capitalize())\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:12.414367Z","iopub.execute_input":"2024-07-25T12:50:12.414822Z","iopub.status.idle":"2024-07-25T12:50:12.431694Z","shell.execute_reply.started":"2024-07-25T12:50:12.414786Z","shell.execute_reply":"2024-07-25T12:50:12.430231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split the data","metadata":{}},{"cell_type":"code","source":"# Split data into training+validation (80%) and test (20%)\nX_train, X_test, y_train, y_test = train_test_split(final_imgs_data, final_labels_data, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:12.433413Z","iopub.execute_input":"2024-07-25T12:50:12.433939Z","iopub.status.idle":"2024-07-25T12:50:12.754243Z","shell.execute_reply.started":"2024-07-25T12:50:12.433896Z","shell.execute_reply":"2024-07-25T12:50:12.753214Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert labels to one-hot encoding\nnum_classes = len(np.unique(labels_data))\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:12.755573Z","iopub.execute_input":"2024-07-25T12:50:12.75593Z","iopub.status.idle":"2024-07-25T12:50:12.765003Z","shell.execute_reply.started":"2024-07-25T12:50:12.755901Z","shell.execute_reply":"2024-07-25T12:50:12.763723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display the train data","metadata":{}},{"cell_type":"code","source":"show_random_images(X_train, y_train,9)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:12.766577Z","iopub.execute_input":"2024-07-25T12:50:12.767048Z","iopub.status.idle":"2024-07-25T12:50:14.866427Z","shell.execute_reply.started":"2024-07-25T12:50:12.767009Z","shell.execute_reply":"2024-07-25T12:50:14.865081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display the test data","metadata":{}},{"cell_type":"code","source":"show_random_images(X_test, y_test,9)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:14.868078Z","iopub.execute_input":"2024-07-25T12:50:14.868572Z","iopub.status.idle":"2024-07-25T12:50:16.880605Z","shell.execute_reply.started":"2024-07-25T12:50:14.868539Z","shell.execute_reply":"2024-07-25T12:50:16.879373Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transfer Learning with MobileNetV2","metadata":{}},{"cell_type":"markdown","source":"### Normalize Data","metadata":{}},{"cell_type":"code","source":"# Normalize your image data\nX_train = np.array(X_train) / 255.0\nX_test = np.array(X_test) / 255.0\ny_train = np.array(y_train)\ny_test = np.array(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:16.882177Z","iopub.execute_input":"2024-07-25T12:50:16.882546Z","iopub.status.idle":"2024-07-25T12:50:19.803964Z","shell.execute_reply.started":"2024-07-25T12:50:16.882514Z","shell.execute_reply":"2024-07-25T12:50:19.80267Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define the MobileNetV2 model","metadata":{}},{"cell_type":"code","source":"base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:19.809137Z","iopub.execute_input":"2024-07-25T12:50:19.809958Z","iopub.status.idle":"2024-07-25T12:50:21.273172Z","shell.execute_reply.started":"2024-07-25T12:50:19.809919Z","shell.execute_reply":"2024-07-25T12:50:21.271966Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Freeze the base model","metadata":{}},{"cell_type":"code","source":"base_model.trainable = False","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:21.275111Z","iopub.execute_input":"2024-07-25T12:50:21.276106Z","iopub.status.idle":"2024-07-25T12:50:21.28602Z","shell.execute_reply.started":"2024-07-25T12:50:21.276064Z","shell.execute_reply":"2024-07-25T12:50:21.284649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create a new model on top","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    Dense(512, activation='relu'),    \n    Dense(num_classes, activation='softmax')  # assuming classification task\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:21.287508Z","iopub.execute_input":"2024-07-25T12:50:21.288246Z","iopub.status.idle":"2024-07-25T12:50:21.303525Z","shell.execute_reply.started":"2024-07-25T12:50:21.288212Z","shell.execute_reply":"2024-07-25T12:50:21.302329Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compile the model","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0001),\n              loss=SparseCategoricalCrossentropy(),\n              metrics=[SparseCategoricalAccuracy()])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:21.305027Z","iopub.execute_input":"2024-07-25T12:50:21.305415Z","iopub.status.idle":"2024-07-25T12:50:21.332175Z","shell.execute_reply.started":"2024-07-25T12:50:21.305385Z","shell.execute_reply":"2024-07-25T12:50:21.330998Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define early stopping callback","metadata":{}},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:21.333703Z","iopub.execute_input":"2024-07-25T12:50:21.334079Z","iopub.status.idle":"2024-07-25T12:50:21.339516Z","shell.execute_reply.started":"2024-07-25T12:50:21.334047Z","shell.execute_reply":"2024-07-25T12:50:21.338229Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"code","source":"# Train the model with early stopping\nhistory = model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:50:21.340992Z","iopub.execute_input":"2024-07-25T12:50:21.341326Z","iopub.status.idle":"2024-07-25T13:03:55.879496Z","shell.execute_reply.started":"2024-07-25T12:50:21.341298Z","shell.execute_reply":"2024-07-25T13:03:55.878165Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluate the model","metadata":{}},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test accuracy: {test_acc}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T13:03:55.881608Z","iopub.execute_input":"2024-07-25T13:03:55.882045Z","iopub.status.idle":"2024-07-25T13:04:36.841633Z","shell.execute_reply.started":"2024-07-25T13:03:55.882012Z","shell.execute_reply":"2024-07-25T13:04:36.839787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting history from training\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_acc = history.history['sparse_categorical_accuracy']\nval_acc = history.history['val_sparse_categorical_accuracy']\n\n# Plotting the training and validation loss\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plotting the training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(train_acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T13:04:36.843942Z","iopub.execute_input":"2024-07-25T13:04:36.844456Z","iopub.status.idle":"2024-07-25T13:04:37.599876Z","shell.execute_reply.started":"2024-07-25T13:04:36.844397Z","shell.execute_reply":"2024-07-25T13:04:37.598547Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test","metadata":{}},{"cell_type":"code","source":"# Make predictions\nindex_test = 110\n# Assuming X_train[0] is the first image in your training data\nimage_to_predict = np.expand_dims(X_test[index_test], axis=0)  # Add batch dimension if necessary\n\n# Make predictions\npredictions = model.predict(image_to_predict)\npredicted_class = np.argmax(predictions[0])\nclasses = {value:key for key,value in labels_.items()}\n# Print predicted class\nprint(f\"Predicted class: {classes[predicted_class]},\\t Real class: {classes[y_test[index_test]]}\")\ndisplay_image(X_test,y_test,index_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}